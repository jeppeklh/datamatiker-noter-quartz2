
## A) Beskriv hvordan I har arbejdet med produktets arkitektur, hvordan I har dokumenteres det og hvilken betydning dette har haft for produktets kvalitet.
Vi har primært dokumenteret arkitekturen med udgangspunkt i 4+1-modellen, og jeg vil komme ind på hvilken betydning dette har haft for produktets kvalitet. 

Jeg vil også reflektere over, hvordan de traditionelle stakeholders-views er blevet tilpasset i vores arbejde, 
og over afvejningen mellem **sporbarhed** og **læsbarhed**.

---

### 4+1-modellen som ramme for arkitekturdokumentation
4+1-modellen opdeler systemarkitekturen i fem views, der hver retter sig mod forskellige stakeholders’ bekymringer:

1. **Logical View** (funktionalitet)
2. **Process View** (runtime, performance)
3. **Development View** (kodeorganisation)
4. **Physical View** (deployment)
5. **Scenarie View** (usecases)

Ved at strukturere vores dokumentation efter disse fem views sikrede vi, at både **high level design**, **low level design** og **kvalitetssikring** blev dækket, selvom vi **manglede** direkte **kontakt** med **kunden**.

---

### Logical View
**Formål** 
Dokumentere systemets kernefunktionalitet for slutbrugere og forretningsmedarbejdere.

**Vores arbejde:**

- Lavet domænemodel, hvor klassernes **koncepter** (fx `VacationOffer`, `Flight`, `Hotel`) og deres **relationer** fremgår.
    
- Forenklede BPMN-diagrammer internt i teamet til at **validere** workflows for søgning, booking og chat.
    

**Tilpasning til mangel på kundeinput:**

- Da vi **ikke** havde direkte adgang til **slutbrugerne**, tog vi i stedet udgangspunkt i kravene fra **kravspecifikationen** som en indirekte repræsentation af deres behov.

- Notationerne i BPMN er forenklede (ikke fuld UML), fordi vi primært brugte diagrammerne **internt** til fælles forståelse.


**Betydning for kvalitet:**

- Fælles forståelse af domænet før implementering sikrede **verificering** i teamet.
    
- Manglende fuld kundevalidering øgede risikoen for misforståelser af systemets krav, men det blev minimeret gennem hyppig intern validering mod kravspecifikation.

---

### Process View
**Formål** 
Vise systemets dynamiske adfærd, concurrency, performance og skalerbarhed.

**Vores arbejde:**
- Sekvensdiagrammer (SD) for realtidschat og asynkrone API-kald til fly- og hotelintegration.
    
- Aktivitetsdiagrammer til at illustrere baggrundstasks, fejlhåndtering og loadscenarier.

**Tilpasning:**

- SD-diagrammerne blev udarbejdet **efter** implementeringen, ford vi vi undervejs havde **svært** ved at forestille os, **hvordan** integrationen skulle fungere i praksis. 
  Vores begrænsede erfaring med API-kald og **usikkerhed** omkring det præcise JSON-format gjorde det **udfordrende** at lave diagrammerne på **forhånd**.

- Selvom det **kunne** **opfattes** som **redundant** at lave SD’er efterfølgende, da vi ikke havde sysadmins eller driftsstakeholders, har diagrammerne haft **intern** **værdi** i teamet. 
  Fx hjalp det én udvikler, der implementerede fly-API’en, med at forklare implementationen for resten af teamet.

- Diagrammerne har også været nyttige til at identificere manglende fejlhåndtering og til at skabe fælles forståelse af flowet.
    

**Betydning for kvalitet:**

- Identifikation af mulige flaskehalse i individuelle API-kald (og API-aggregation - måske senere).
    
- Afdækning af behov for kø-mekanismer (async og håndtering af concurrency) forbedrede systemets robusthed.

---

### Development View
**Formål** 
Visualisere kodebasens struktur, moduler, lag og komponenter for udviklere og projektledelse.

**Vores arbejde:**

- Package-diagrammer  med moduler: UI/Præsentation (Blazor), Applikationsservices, Dataaccess (Repository, EF DbContext) og Integrationslag.

- DCD - forsimplet.

**Tilpasning:**

- Udeladelse af enkelte attributter og relationer i diagrammerne for at bevare overblik og læsbarhed.
    
- Fokus på komponentafhængigheder og deploymentscope frem for fuldstændig sporbarhed.
    

**Sporbarhed vs. læsbarhed:**
Forenklingen reducerede **sporbarheden**, men valget var **bevidst** som en **balance** mellem **læsbarhed** og **kvalitetssikring**. 
Den fælles forståelse i teamet sikrede fortsat **verificering**, mens **validering** mod **kravspecifikationen** bevarede relevansen.

**Betydning for kvalitet:**

- Klare lag-delinger og færre dependencies øgede modularitet og testbarhed.
    
- Forenklede diagrammer hjalp læsbarhed. 
  Gav overblik - det var det vi hovedsagligt brugte værktøjet til. Vi sad hele teamet sammen når der blev drøftet evt. ændringer. Dvs. at de implicitte ting, var grundigt drøftet af hele teamet.

---

### Physical View
**Formål** 
Kortlægge softwarekomponenter til hardware og infrastruktur for systemadministratorer og tekniske ledere.

**Vores arbejde:**

- Deployment-diagram med Azure App Service, (indbygget Load Balancer?), Azure SQL Database og SignalR-endpoints.
    
- Beskrivelse af CI/CD-pipeline i Azure DevOps.
    

**Tilpasning:**

- Udeladelser af detaljer om netværk og firewall-regler, da vi ikke havde adgang til virksomhedens driftsmiljø.
    

**Betydning for kvalitet:**

- Sikring af redundans og high availability ved at beskrive flere instanser og failover-mekanismer. (at der ikke er dobbelte sql servers, og SignalR-Connections)
    
- CI/CD-dokumentation gjorde deployments mere forudsigelige og gennemskuelige (alt er testet og kan køres).

---

### Scenarie View
**Formål** 
**Validere** de fire andre views gennem konkrete **usecases** eller scenarier.

**Vores arbejde:**

- Vi havde faktisk arbejdet med mange usecases/scenarier undervejs, men hvis vi kigger på en **udvalgt central usecase**: 
	  - Booking-flow (medarbejder - søg, vælg, sammensæt, præsenter) 

Blev til feature
- Hver **feature** dækkede ofte **flere usecases**, men den her blev brugt som et eksempel til at skabe overblik.
[[4+1 (ny)#Scenarie View (usecases)|Scenarie view generelt]]

**Tilpasning:**

- Brugen af **scenarier i modellen** har primært været **retrospektiv**, altså noget vi først dokumenterede til sidt. Men vi har undervejs **underforstået** brugt elementer af scenarietænkning i udviklingsprocessen. (**implicit validering**)
    
- **Usecases** blev kun **dokumenteret internt** og tog udgangspunkt i **kravspecifikationens** eksempler, da **brugertests** **ikke** var **mulige**.
    

**Betydning for kvalitet:**

- Usecases afslørede blandt andet manglende fejlhåndtering i booking-flowet (f.eks. ved API-fejl).
    
- Det fungerede som et bindeled mellem krav og implementering, og var nyttigt i at kommunikere og forstå flows internt i teamet.

---

### Overordnet betydning for produktets kvalitet
**Fælles sprog i teamet** 
4+1-modellen skabte et fælles referenceramme, hvor vi kunne diskutere arkitektur uden at misforstå hinanden.

**Modularitet og vedligeholdelse** 
Klart dokumenterede views bidrog til en modulopbygget kodebase, hvilket gjorde det nemmere at tilføje nye features.

**Tidlig identifikation af risici** 
Process- og scenarie-view understøttede identifikation af performance- og fejlhåndteringsrisici tidligt i projektet.

**Balance mellem sporbarhed og læsbarhed** 
Ved at tilpasse UML-diagrammerne til teamets behov sikrede vi hurtig forståelse uden at drukne i detaljer.


Samlet set har arbejdet med 4+1‑modellen ikke blot dokumenteret arkitekturen, men aktivt understøttet vores kvalitetssikring gennem hele udviklingsforløbet – både hvad angår funktionalitet, robusthed og teamkoordinering.

[[1. Spørgsmål#B) Beskriv 4 + 1 modellen og forklar hvad den kan anvendes til| 4+1 B spørgsmål]]

---

## B) Beskriv forskellige typer af test og test niveauer samt hvornår, hvordan og hvorfor disse anvendes. Derudover skal du redegøre for væsentlige forskelle i test ved agile og vandfaldsmodeller
 Test bruges **ikke** kun til at finde fejl, men er en **systematisk proces**, der sikrer, at produktet **lever op til krav**, design og brugernes forventninger. 
 
Gennem forskellige **testtyper** og **testniveauer** undersøger vi både funktionalitet, samspil mellem komponenter og kvalitet i koden – fra overordnet systemadfærd til den enkelte linje kode.

---

### Testtyper
2 overordnede testyper:

**Funktionelle tests**  
**Validerer** systemets funktionalitet mod specificerede **krav**  
- fx unittests, integrationstests, systemtests og accepttests.

**Ikke-funktionelle tests**  
Måler ydeevne, sikkerhed, usability og skalerbarhed  
-  fx load-, stress- og penetrationstest.

**Regressionstest**  
Sikrer, at ny kode **ikke** introducerer fejl i **eksisterende funktionalitet**.  
Ofte **baseret** på **automatiserede** tests, som **genkøres** ved **hver ændring**

---

### Testniveauer
#### Unit-test (programmeringsniveau, white-box)

**Formål** 
**Validere** den mindste enhed af kode (funktion/metode/klasse), sikre korrekt opførsel, best practices og høj code coverage.  

**Perspektiv** 
White-box – vi **kender** implementationen og tester **interne strukturer**.  

**Metoder:**
- Unittest-framework (JUnit, NUnit)
- Statisk kodeanalyse (linting, SonarQube)
- Code coverage (>80 %)  

**Eksempel** 
En unittest af en **e-mail-valideringsfunktion**, hvor både gyldige og ugyldige formater testes systematisk (jf. læringsmateriale om Validering og Verificering).
    

#### Integrationstest (designniveau, grey-box)

**Formål** 
Sikre at komponenter/moduler **samarbejder** efter **arkitektur** og **kontrakt**.  

**Perspektiv** 
Grey-box – kendskab til interfaces/arkitektur.  

**Metoder:**
- API-kontrakttest med **mocks** eller **testdatabaser**
- Databasedrevet **integrationstest**
- **Load**- og **stress**-test mod servicegrænseflader  

**Eksempel** 
Test af **betalingsmodulet** mod et eksternt **regnskabssystem**:    
1. Betalingsmodulet sender JSON: `{"amount":100.00, "currency":"DKK", "orderId":"1234"}`
2. Mock-regnskabssystem bekræfter korrekte felter.
3. Måler svartider og fejlrater ved 100 samtidige requests i staging-miljø
    

#### Systemtest (konceptuelt niveau, black-box)

**Formål** 
**Validere** **hele** systemets **funktionalitet** mod **kravspecifikation** og design **uden** interne detaljer.  

**Perspektiv** 
Black-box – udelukkende kravbaseret.  

**Metoder:**
- Funktionelle tests baseret på use cases
- Ikke-funktionelle tests: performance, sikkerhed, usability
    
- Regressionstest: genkørsel af tidligere testcases (automatisk) for at sikre, at ændringer ikke bryder eksisterende funktionalitet.

#### Accepttest (konceptuelt niveau, black-box)

**Formål**
**Slutbrugervalidering** mod forretningskrav – “gør vi de rigtige ting?” (**Validering**).  

**Perspektiv** 
Black-box, **brugercentreret**.  

**Metoder:**
- User Acceptance Testing (UAT) med rigtige brugere eller produktansvarlige
- Kriterier baseret på user stories eller kravmatricer  

**Eksempel** 
En UserAcceptTest-session for rabatkodeflow:
Tilføj varer, anvend “SUMMER2025”, **verificer** korrekt rabat og intuitivt checkout (jf. læringsmateriale om UAT).

---

### Validering og verificering af test
**Validering ("gør vi de rigtige ting?")**

- Her vurderer man, om de testcases, der er opstillet, **dækker** de relevante **krav** og **brugerbehov**.

- **Sporbarhed** kan sikres gennem **tydelige** **referencer** fra **krav** til **tests** 
  fx ved at **nævne** **krav** i testbeskrivelser.

**Eksempel** 
En funktion som "Filtrer søgning på kategori" skal have relevante tests, som dækker både synlighed af filter, filtreringens præcision og eventuel fejlvisning ved forkert input.

**Verificering ("gør vi tingene rigtigt?")**

- Her undersøges det, om de **testcases** er **korrekt** **implementeret** og **fungerer** efter hensigten.

- Kan fx ske gennem **code reviews** af testkoden, kontrol af testmiljø (f.eks. mock-opkoblinger og testdata) og brug af testanalyseværktøjer.

**Eksempel** 
Et **teammedlem** **vurderer**, om en unittest **faktisk** **tester** hele metodeforløbet – og om assertions **matcher** **forretningsregler** og fejlhåndtering.

---

### Test i agile vs. vandfaldsmodeller
Test håndteres forskelligt afhængigt af, om man følger en vandfalds- eller agil procesmodel.

---

#### Vandfaldsmodellen
**Faseopdelt udvikling** 
**Testfase** er typisk en **selvstændig** fase, der først udføres **efter** **implementeringsfasen**. 
Det betyder, at fejl kun **opdages** i en **senere** fase, hvilket kan medføre **dyrere** **fejlrettelser**.

**Dokumentation og planlægning** 
Detaljerede testplaner og -specifikationer, **god** **sporbarhed**, men **begrænset** **fleksibilitet**.   

**Sekventiel tilgang** 
Hver testfase (unit, integration, system, accepttest) udføres i rækkefølge, og der er lille mulighed for feedback til designet under testfasen.
   
**Fordele** 
Man ved på forhånd, hvornår test starter og slutter.

**Ulemper** 
Fejl opdaget **sent** med høje rettelsesomkostninger og **ringe** **feedback** til designet under test.  

Hvis **kravene** er **kendte** fra starten og **ikke ændrer** sig, fungerer testfasen i vandfaldsmodellen effektivt.

---

#### Iterativ og inkrementel (agil) tilgang 
Test udføres **løbende** under hele **udviklingscyklussen**. 
Unit- og integrationstest køres typisk automatisk i CI/CD-pipeline

**Automatiseret testning** 
Der lægges stor **vægt** på **automatiseret testning** for at sikre hurtig **feedback** til **udviklere**, hvilket understøtter kontinuerlig integration og deployment.

**Fokus på samarbejde og feedback** 
Tæt **samarbejde** mellem **udviklere**(testere) og **kunden** sikrer, at der **tidligt** **rettes** op på eventuelle **problemer**, og at produktet lever op til brugerens forventninger. (**validering**)

**Fordele** 
Tidlig fejlidentifikation, fleksibilitet, kontinuerlig **kvalitetssikring**.

**Ulemper**
Løbende ændringer kan gøre det **svært** at opretholde **omfattende dokumentation**, medmindre der etableres klare retningslinjer for dokumentationsstyring.
**Hyppige ændringer i krav** og kode betyder, at testcases **konstant** skal **opdateres**.


[[9. Spørgsmål#A) Beskriv hvilke test I har udført i jeres projekt. Hvad testede I? Hvad blev ikke testet. Forklar hvorfor og reflekter over konsekvenserne.|Test A spørgsmål]]


